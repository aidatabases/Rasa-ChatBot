{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "everyday-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affiliated-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"final_data.csv\")\n",
    "sent_to_id  = {\"empty\":0, \"sadness\":1,\"excited\":2,\"fear\":3,\n",
    "                        \"surprise\":4,\"disgust\":5,\"happiness\":6,\"anger\":7}\n",
    "contractions = pd.read_csv(\"contractions.csv\")\n",
    "cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surprising-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_data = pd.read_csv(\"aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n",
    "misspell_data.misspell = misspell_data.misspell.str.strip()\n",
    "misspell_data.misspell = misspell_data.misspell.str.split(\" \")\n",
    "misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\n",
    "misspell_data.drop_duplicates(\"misspell\",inplace=True)\n",
    "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n",
    "\n",
    "#Sample of the dict\n",
    "#{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incorporate-laundry",
   "metadata": {
    "id": "acknowledged-drunk"
   },
   "outputs": [],
   "source": [
    "def misspelled_correction(val):\n",
    "    for x in val.split(): \n",
    "        if x in miss_corr.keys(): \n",
    "            val = val.replace(x, miss_corr[x]) \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "northern-vienna",
   "metadata": {
    "id": "sharing-township"
   },
   "outputs": [],
   "source": [
    "def cont_to_meaning(val): \n",
    "  \n",
    "    for x in val.split(): \n",
    "        if x in cont_dic.keys(): \n",
    "            val = val.replace(x, cont_dic[x]) \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dedicated-reflection",
   "metadata": {
    "id": "continued-sally"
   },
   "outputs": [],
   "source": [
    "def punctuation(val): \n",
    "  \n",
    "    punctuations = '''()-[]{};:'\"\\,<>./@#$%^&_~'''\n",
    "  \n",
    "    for x in val.lower(): \n",
    "        if x in punctuations: \n",
    "            val = val.replace(x, \" \") \n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disabled-gardening",
   "metadata": {
    "id": "disciplinary-athens"
   },
   "outputs": [],
   "source": [
    "def clean_text(val):\n",
    "    val = misspelled_correction(val)\n",
    "    val = cont_to_meaning(val)\n",
    "    #val = p.clean(val)\n",
    "    val = ' '.join(punctuation(emoji.demojize(val)).split())\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "plain-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    token = pickle.load(handle)\n",
    "batch_size = 32\n",
    "model = load_model('my_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjacent-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 160\n",
    "def output2(text1):\n",
    "    text = clean_text(text1)\n",
    "    twt = token.texts_to_sequences([text1])\n",
    "    twt = sequence.pad_sequences(twt, maxlen=max_len, dtype='int32')\n",
    "    sentiment = model.predict(twt,batch_size=1,verbose = 2)\n",
    "    result = pd.DataFrame([sent_to_id.keys(),sentiment[0]]).T\n",
    "    result=result.dropna()\n",
    "    result.columns = [\"sentiment\",\"percentage\"]\n",
    "    result=result.T\n",
    "    result.columns = result.iloc[0]\n",
    "    result.drop(['sentiment'],axis=0,inplace=True)\n",
    "    result=result.iloc[:1]\n",
    "    r = result.to_numpy().tolist()\n",
    "    r=r[0]\n",
    "    return r\n",
    "    \n",
    "#     new_header = result.iloc[0] #grab the first row for the header\n",
    "#     result = result[1:] #take the data less the header row\n",
    "#     result.columns = new_header\n",
    "    #return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abroad-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010666491,\n",
       " 0.5506197,\n",
       " 0.0038057922,\n",
       " 0.2927482,\n",
       " 0.05176571,\n",
       " 0.08408123,\n",
       " 0.0043951804,\n",
       " 0.0019175473]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2('As early as I could remember myself, my memories have been colored with melancholy and sad feelings. Yes, I admit it: I am a person who loves being sad. As a child, I loved gloomy fairy tales; as a teenager, I loved tragic novels and films, which made my parents worry a lot. I realized that I did not meet their expectations: in their ideal world, a happy person must irradiate joy all the time. In my view, this state of constant life enjoyment looked slightly idiotic. And I kept asking myself if sadness is really so bad.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-begin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file =pd.read_excel(\"Sample Data File.xlsx\",sheet_name='Formulas')\n",
    "# file.drop(file.iloc[:, 1:], inplace = True, axis = 1)\n",
    "# file.dropna(inplace=True)\n",
    "# file['new']=file['text'].apply(output2)\n",
    "# file[['empty','sadness','excited','fear','surprise','disgust','happiness','anger']] = pd.DataFrame(file.new.tolist(),index= file.index)\n",
    "# file.drop(columns={'new'},inplace=True)\n",
    "# file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file[['empty','sadness','excited','fear','surprise','disgust','happiness','anger']] = pd.DataFrame(file.new.tolist(),index= file.index)\n",
    "# file.drop(columns={'new'},inplace=True)\n",
    "# file"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "first_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
